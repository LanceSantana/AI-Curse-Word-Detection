{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HmDg-ACG6p6"
      },
      "source": [
        "# Import and Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OqCcc0Qaaqq",
        "outputId": "ac9f2b0a-1c75-4914-a2d1-1a5e96ece35e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8JqRXIO2zGs",
        "outputId": "ba19d713-555e-478e-d7f4-734d94d91828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting better_profanity\n",
            "  Downloading better_profanity-0.7.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Downloading better_profanity-0.7.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: better_profanity\n",
            "Successfully installed better_profanity-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install better_profanity\n",
        "from better_profanity import profanity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "import string\n",
        "\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "EthP8ME1ksMZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1897bcf2-b69e-4d84-ca58-41c31832e56e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhzymzZPtCc9",
        "outputId": "717671ef-3e1e-4004-bb6d-1a368ef38bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRHPaPFCSp1A"
      },
      "source": [
        "# Data Collection and Cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "LLGC-posNBBr",
        "outputId": "f6ac7e88-c741-4ea1-958b-7751100dc285"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [subreddit, body]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a63d2242-77ab-44b1-8782-52fb08df7484\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subreddit</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a63d2242-77ab-44b1-8782-52fb08df7484')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a63d2242-77ab-44b1-8782-52fb08df7484 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a63d2242-77ab-44b1-8782-52fb08df7484');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_dce3a690-871f-4a66-a30b-cf4d4c6e6324\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_dce3a690-871f-4a66-a30b-cf4d4c6e6324 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# load reddit data into dataframe\n",
        "df = pd.read_csv(\"reddit_40.csv\", skiprows=range(1,50000), nrows=2000)\n",
        "\n",
        "df = df.drop(\"controversiality\", axis=1)\n",
        "df = df.drop(\"score\", axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EKnfDknikXuZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c2a99a-ba99-40d2-ac7d-ee3eecab6b4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hl3sOf2UPIli",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "79303abc-306b-46c5-a424-4f5822fdb2b0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'body'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'body'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-79823762f250>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.9\u001b[0m  \u001b[0;31m# 90% ASCII\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"body\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_mostly_english\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'body'"
          ]
        }
      ],
      "source": [
        "# get rid of all non-english text (row)\n",
        "\n",
        "def is_mostly_english(text):\n",
        "    return sum(1 for c in text if ord(c) < 128) / len(text) > 0.9  # 90% ASCII\n",
        "\n",
        "df = df[df[\"body\"].apply(lambda x: isinstance(x, str) and is_mostly_english(x))]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNEcDCHSULZQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "d7528ebd-241a-4b3e-e50e-dee292bd82fe"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'body'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'body'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-c35f4a37c7a1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#get rid of URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"body\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"body\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"http\\S+|www\\S+|https\\S+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMULTILINE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'body'"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "#get rid of URLs\n",
        "df[\"body\"] = df[\"body\"].apply(lambda x: re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", x, flags=re.MULTILINE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIuSog-uUSC6"
      },
      "outputs": [],
      "source": [
        "# Remove special characters and excess whitespace\n",
        "df[\"body\"] = df[\"body\"].apply(lambda x: re.sub(r\"[^A-Za-z0-9.,!?'\\s]\", \"\", x))  # Keep only letters, numbers, and some punctuation\n",
        "df[\"body\"] = df[\"body\"].apply(lambda x: \" \".join(x.split()))  # Remove extra spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyO_WxNHUsPl"
      },
      "outputs": [],
      "source": [
        "# convert to lowercase\n",
        "df[\"body\"] = df[\"body\"].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e27lIJm8U2T1"
      },
      "outputs": [],
      "source": [
        "#Remove duplicate comments\n",
        "df = df.drop_duplicates(subset=[\"body\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnbL79fXU-xH"
      },
      "outputs": [],
      "source": [
        "# remove rows where comment is missing\n",
        "df = df.dropna(subset=[\"body\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9hblnBqSwyD"
      },
      "source": [
        "#Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx4CfCIg24ES"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Apply the profanity detection\n",
        "df['profanity_check'] = df['body'].apply(lambda x: 1 if profanity.contains_profanity(x) else 0)\n",
        "\n",
        "# Check the output\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTvbhs3grba0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Count the number of rows with and without profanity\n",
        "profanity_counts = df['profanity_check'].value_counts()\n",
        "\n",
        "# Labels for the pie chart\n",
        "labels = ['No Profanity', 'Profanity']\n",
        "\n",
        "# Plot the pie chart\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.pie(profanity_counts, labels=labels, autopct='%1.1f%%', colors=['lightblue', 'red'], startangle=140)\n",
        "plt.title('Proportion of Profane vs. Non-Profane Texts')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEh7wliWimnf"
      },
      "outputs": [],
      "source": [
        "# Count profanity occurrences by subreddit\n",
        "subreddit_counts = df.groupby('subreddit')['profanity_check'].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=subreddit_counts.index, y=subreddit_counts.values, palette=\"Reds\")\n",
        "plt.xlabel(\"Subreddit\")\n",
        "plt.ylabel(\"Profane Messages Count\")\n",
        "plt.title(\"Top 10 Subreddits with Most Profane Messages\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Awp4rmPaoTTa"
      },
      "outputs": [],
      "source": [
        "with open('bad-words.txt', 'r') as f:\n",
        "   swear_words = [line.strip().lower() for line in f]\n",
        "\n",
        "def count_profanity(text):\n",
        "   words = text.split()\n",
        "   profanity_count = sum(1 for word in words if word in swear_words)\n",
        "   return profanity_count\n",
        "\n",
        "df['profanity_count'] = df['body'].apply(count_profanity)\n",
        "\n",
        "# Count occurrences of each swear word\n",
        "profanity_counts = Counter()\n",
        "for text in df['body']:\n",
        "   for word in text.split():\n",
        "     if word in swear_words:\n",
        "       profanity_counts[word]+=1\n",
        "\n",
        "# Get the 10 most frequent profanity words\n",
        "most_common_profanity = profanity_counts.most_common(10)\n",
        "\n",
        "# Extract words and counts for plotting\n",
        "words, counts = zip(*most_common_profanity)\n",
        "\n",
        "# Generate word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(profanity_counts)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud of Profanity Words in Reddit Comments\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFQEpldHD-Ns"
      },
      "source": [
        "# Technique 1: Profanity Censoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BN7emEFEKoX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d8c92dd-65ea-4dd3-d0f8-be60518e2820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "This is some crazy ****!\n"
          ]
        }
      ],
      "source": [
        "print(profanity.contains_profanity(\"This is some crazy shit!\"))  # Testing out 1st: should output True\n",
        "censored_comment = profanity.censor(\"This is some crazy shit!\")\n",
        "print(censored_comment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVpsx_B0ECiS"
      },
      "outputs": [],
      "source": [
        "def contains_profanity(comment):\n",
        "    return profanity.contains_profanity(comment)\n",
        "\n",
        "def censor_profanity(comment):\n",
        "    return profanity.censor(comment)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['has_profanity'] = df['body'].apply(contains_profanity) #do the same with the whole file; makes new columns\n",
        "\n",
        "#df.to_csv(\"reddit_censored.csv\", index=False)"
      ],
      "metadata": {
        "id": "0Z8bJRNYVVZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_body'] = df['body'].apply(censor_profanity) #do the same with the whole file; makes new columns"
      ],
      "metadata": {
        "id": "7MV-dAVyVYb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['profanity_check']==1][['body', 'clean_body']]"
      ],
      "metadata": {
        "id": "KqyXJ_5Ng4zB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['results_check'] = df['clean_body'].apply(lambda x: 1 if profanity.contains_profanity(x) else 0)\n",
        "df[['body', 'clean_body', 'results_check']]\n"
      ],
      "metadata": {
        "id": "urC0llk8h_tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profanity_count = df['results_check'].sum()\n",
        "print(f\"Number of rows with profanity: {profanity_count}\")"
      ],
      "metadata": {
        "id": "s2NODdyTlmyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lanIrGmxCgR"
      },
      "source": [
        "# Technique 2: KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11BWDwIskhrI"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-7ovGXMTaku"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to convert NLTK POS tags to WordNet POS tags\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):  # Adjectives\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):  # Verbs\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):  # Nouns\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):  # Adverbs\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun\n",
        "\n",
        "# Function to lemmatize text with POS tagging\n",
        "def lemmatize_text(text):\n",
        "    if pd.isna(text):  # Handle missing values\n",
        "        return \"\"\n",
        "\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
        "    tokens = word_tokenize(text)  # Tokenize text\n",
        "    pos_tags = nltk.pos_tag(tokens)  # Get POS tags\n",
        "\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
        "    return \" \".join(lemmatized_words)  # Reconstruct text\n",
        "\n",
        "\n",
        "# Apply lemmatization to the 'comments' column\n",
        "df[\"lemmatized_body\"] = df[\"body\"].astype(str).apply(lemmatize_text)\n",
        "\n",
        "\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR5fVWbFYKOA"
      },
      "source": [
        "## Stop Word Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqqPHLCcYJqT"
      },
      "outputs": [],
      "source": [
        "#imports and downloads stop words\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "#creates a set of stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define a function to remove stop words\n",
        "def remove_stopwords(text):\n",
        "    # Tokenize the text into individual words\n",
        "    tokens = word_tokenize(text)\n",
        "    #filters out stopwords and non-letters\n",
        "    non_stopword_tokens = [token for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
        "    return ' '.join(non_stopword_tokens)\n",
        "\n",
        "# Apply remove stopwords to the 'lemmatized_body' column\n",
        "df['stopwords_lemmatized_body'] = df['lemmatized_body'].apply(remove_stopwords)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHMkOpbsXxI-"
      },
      "source": [
        "##Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wy1rWyS7X0tn"
      },
      "outputs": [],
      "source": [
        "#improt and create count vectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "#fits tokens to vocabulary\n",
        "bow_matrix = vectorizer.fit_transform(df[\"stopwords_lemmatized_body\"])\n",
        "\n",
        "#converts bow matrix into array for viewing\n",
        "bow_array = bow_matrix.toarray()\n",
        "bow_names = vectorizer.get_feature_names_out()\n",
        "bow_df = pd.DataFrame(bow_array, columns = bow_names)\n",
        "\n",
        "#displays first rows of bow data f(rame\n",
        "bow_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTJKxpcTYDSb"
      },
      "source": [
        "##TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxDvA7HXms7g"
      },
      "outputs": [],
      "source": [
        "#import TFIDF vectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "#initalize a tfidf transformer\n",
        "tfidf_transform = TfidfTransformer()\n",
        "\n",
        "#apply the transformer to the bow matrix\n",
        "tfidf_matrix = tfidf_transform.fit_transform(bow_matrix)\n",
        "\n",
        "#convert the tfidf matrix in a dense array\n",
        "tfidf_array = tfidf_matrix.toarray()\n",
        "\n",
        "#create a data frame for viewing\n",
        "tfidf_df = pd.DataFrame(tfidf_array, columns= bow_names)\n",
        "\n",
        "#display\n",
        "tfidf_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN Model"
      ],
      "metadata": {
        "id": "lzu_SyetlfDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def knn_profanity_replacement(input_df, k=3):\n",
        "\n",
        "    profane_sentences = input_df[input_df['has_profanity'] == True]['stopwords_lemmatized_body'].tolist()\n",
        "    clean_sentences = input_df[input_df['has_profanity'] == False]['stopwords_lemmatized_body'].tolist()\n",
        "\n",
        "    all_sentences = profane_sentences + clean_sentences\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    sentence_vectors = vectorizer.fit_transform(all_sentences)\n",
        "\n",
        "    profane_vectors = sentence_vectors[:len(profane_sentences)]\n",
        "    clean_vectors = sentence_vectors[len(profane_sentences):]\n",
        "\n",
        "    knn = NearestNeighbors(n_neighbors=k, metric='cosine')\n",
        "    knn.fit(clean_vectors)\n",
        "\n",
        "    nearest_clean_indices = knn.kneighbors(profane_vectors, return_distance=False)\n",
        "\n",
        "    replaced_sentences = []\n",
        "    for idx, profane_sentence in enumerate(profane_sentences):\n",
        "        nearest_clean_sentence = clean_sentences[nearest_clean_indices[idx][0]]\n",
        "\n",
        "        profane_words = profane_sentence.split()\n",
        "        clean_words = nearest_clean_sentence.split()\n",
        "\n",
        "        replaced_sentence = \" \".join([\n",
        "            clean_words[i] if profanity.contains_profanity(word) else word\n",
        "            for i, word in enumerate(profane_words)\n",
        "            if i < len(clean_words)\n",
        "        ])\n",
        "\n",
        "        replaced_sentences.append(replaced_sentence)\n",
        "\n",
        "    input_df.loc[input_df['has_profanity'] == True, 'replaced_body'] = replaced_sentences\n",
        "\n",
        "    return input_df\n",
        "\n",
        "df = knn_profanity_replacement(df)\n",
        "df[['body', 'replaced_body']]"
      ],
      "metadata": {
        "id": "J-lt83G3JNzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEnwj_bsoafM"
      },
      "source": [
        "\n",
        "\n",
        "# Technique 3: Synonym Comparison Replacement"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "pCwznIVSJq5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors"
      ],
      "metadata": {
        "id": "4cuz4kA8JrH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Path to the model in Google Drive\n",
        "model_path = '/content/drive/MyDrive/GoogleNews-vectors-negative300.bin'\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)\n"
      ],
      "metadata": {
        "id": "iUotEGn8Juw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from better_profanity import profanity\n",
        "from nltk.corpus import wordnet\n",
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Ensure required NLTK data is downloaded\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Load pre-trained Word2Vec model (adjust path as needed)\n",
        "word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
        "\n",
        "# Helper function to map POS tags from WordNet to a simpler format\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to the WordNet POS tag.\"\"\"\n",
        "    from nltk.corpus import wordnet as wn\n",
        "    tag = nltk.pos_tag([word])[0][1]\n",
        "    if tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Function to get synonyms from WordNet, filtered by POS\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    wordnet_pos = get_wordnet_pos(word)  # Get POS of the profane word\n",
        "    for syn in wordnet.synsets(word, pos=wordnet_pos):  # Filter by POS\n",
        "        for lemma in syn.lemmas():\n",
        "            if lemma.name().lower() != word.lower():  # Avoid returning the word itself\n",
        "                synonyms.add(lemma.name().replace(\"_\", \" \"))\n",
        "    return list(synonyms)\n",
        "\n",
        "# Function to get the best non-profane synonym using KNN\n",
        "def best_synonym(profane_word, censor=True):\n",
        "    synonyms = get_synonyms(profane_word)\n",
        "\n",
        "    if not synonyms:\n",
        "        return \"*\" * len(profane_word) if censor else \"[CENSORED]\"  # Replace with asterisks\n",
        "\n",
        "    # Remove profane synonyms\n",
        "    synonyms = [word for word in synonyms if not profanity.contains_profanity(word)]\n",
        "\n",
        "    if not synonyms:\n",
        "        return \"*\" * len(profane_word) if censor else \"[CENSORED]\"  # Replace with asterisks\n",
        "\n",
        "    # Get word vectors\n",
        "    word_vectors = []\n",
        "    valid_synonyms = []\n",
        "\n",
        "    if profane_word in word2vec_model:\n",
        "        profane_vector = word2vec_model[profane_word]\n",
        "    else:\n",
        "        return \"*\" * len(profane_word) if censor else \"[CENSORED]\"  # Replace with asterisks\n",
        "\n",
        "    for synonym in synonyms:\n",
        "        if synonym in word2vec_model:\n",
        "            word_vectors.append(word2vec_model[synonym])\n",
        "            valid_synonyms.append(synonym)\n",
        "\n",
        "    if not valid_synonyms:\n",
        "        return \"*\" * len(profane_word) if censor else \"[CENSORED]\"  # Replace with asterisks\n",
        "\n",
        "    # Convert to NumPy array\n",
        "    word_vectors = np.array(word_vectors)\n",
        "\n",
        "    # Use KNN to find the closest synonym\n",
        "    knn = NearestNeighbors(n_neighbors=1, metric='cosine')\n",
        "    knn.fit(word_vectors)\n",
        "    distances, indices = knn.kneighbors([profane_vector])\n",
        "\n",
        "    return valid_synonyms[indices[0][0]]\n",
        "\n",
        "# Function to clean text in a DataFrame column\n",
        "def clean_text(text):\n",
        "    words = text.split()\n",
        "    cleaned_words = [best_synonym(word) if profanity.contains_profanity(word) else word for word in words]\n",
        "    return \" \".join(cleaned_words)\n",
        "\n",
        "# Apply the function to the DataFrame's text column\n",
        "df[\"cleaned_body\"] = df[\"body\"].astype(str).apply(clean_text)\n",
        "\n",
        "df[df[\"profanity_check\"] == 1][[\"body\", \"cleaned_body\"]]"
      ],
      "metadata": {
        "id": "q391Ux3pczBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['results_check'] = df['cleaned_body'].apply(lambda x: 1 if profanity.contains_profanity(x) else 0)"
      ],
      "metadata": {
        "id": "oyHZUTkxkqJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['profanity_check']== 1][['body', 'profanity_check', 'cleaned_body', 'results_check']]"
      ],
      "metadata": {
        "id": "cisL03WzmV8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profanity_count = df['results_check'].sum()\n",
        "print(f\"Number of rows with profanity: {profanity_count}\")"
      ],
      "metadata": {
        "id": "hRnNLrLtlWSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n"
      ],
      "metadata": {
        "id": "3_ZLdjKld-6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technique 1: Profanity Censoring"
      ],
      "metadata": {
        "id": "qRx32pkReOCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Technique 2: KNN\n",
        "\n"
      ],
      "metadata": {
        "id": "ljDdGHSylD0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technique 3: Synonym Replacement"
      ],
      "metadata": {
        "id": "_vCLgNXUlvDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "zLy8qMYRl0zD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cGFjMYHpl3xL"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "lR5fVWbFYKOA",
        "GHMkOpbsXxI-",
        "NTJKxpcTYDSb"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}